{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils.create_features_utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import metrics\n",
    "from keras.models import load_model\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read match data with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/wimbledon_matches_with_feature.csv')\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df['diff_rank'] = df['player_0_rank'] - df['player_1_rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [\n",
    " 'diff_rank',\n",
    " 'diff_match_win_percent',\n",
    " 'diff_games_win_percent',\n",
    " 'diff_5_set_match_win_percent',\n",
    " 'diff_close_sets_percent',\n",
    " 'diff_match_win_percent_grass',\n",
    " 'diff_games_win_percent_grass',\n",
    " 'diff_5_set_match_win_percent_grass',\n",
    " 'diff_close_sets_percent_grass',\n",
    " 'diff_match_win_percent_52',\n",
    " 'diff_games_win_percent_52',\n",
    " 'diff_5_set_match_win_percent_52',\n",
    " 'diff_close_sets_percent_52',\n",
    " 'diff_match_win_percent_grass_60',\n",
    " 'diff_games_win_percent_grass_60',\n",
    " 'diff_5_set_match_win_percent_grass_60',\n",
    " 'diff_close_sets_percent_grass_60',\n",
    " 'diff_match_win_percent_hh',\n",
    " 'diff_games_win_percent_hh',\n",
    " 'diff_match_win_percent_grass_hh',\n",
    " 'diff_games_win_percent_grass_hh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data intro Train (80 %) and Test (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.outcome\n",
    "features = df[features_list]\n",
    "\n",
    "train_features, test_features, train_target, test_target = train_test_split(features, target, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the neural network. \n",
    "### Deatils\n",
    "    - Number of Layers: 3. (2 Hidden Layers)\n",
    "    - Number of Neuros in each layer: 64->32->1\n",
    "    - Activation relu->relu->sigmoid\n",
    "    - Stop if validation loss does not improve for 500 epochs\n",
    "    - Save the best model which gives the maximum validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.25234, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.25234 to 0.61772, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.61772\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.61772\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61772\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61772 to 0.59556, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.59556\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.59556 to 0.56623, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.56623\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.56623 to 0.56397, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.56397\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.56397 to 0.55723, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.55723\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.55723 to 0.55266, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.55266 to 0.55077, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.55077 to 0.54792, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.54792 to 0.54413, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.54413\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.54413\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.54413\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.54413 to 0.53564, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.53564 to 0.53436, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.53436\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.53436\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.53436 to 0.52983, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.52983\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.52983\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.52983\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.52983 to 0.52528, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.52528\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.52528 to 0.52256, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.52256\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.52256\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.52256\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.52256\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.52256 to 0.52058, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.52058\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.52058 to 0.51755, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51755\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51755\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51755\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51755\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.51755 to 0.51468, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51468\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51468\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.51468 to 0.51248, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51248\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.51248 to 0.51145, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51145\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.51145 to 0.51069, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51069\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51069\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.51069 to 0.50739, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.50739\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.50739\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.50739\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.50739\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.50739\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.50739\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.50739\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.50739 to 0.50673, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.50673\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.50673\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.50673\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.50673\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.50673\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.50673\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.50673\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.50673\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.50673 to 0.50615, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.50615\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.50615 to 0.50418, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.50418\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.50418\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.50418\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.50418\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.50418\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.50418\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.50418\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.50418\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.50418 to 0.50030, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.50030\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.50030\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.50030 to 0.49924, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.49924\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.49924 to 0.49813, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.49813\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.49813 to 0.49616, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.49616\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.49616 to 0.49572, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.49572\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.49572\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.49572\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.49572\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.49572\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.49572\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.49572 to 0.49533, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.49533\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.49533\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.49533\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.49533\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.49533\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.49533\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.49533 to 0.49527, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.49527\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.49527\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.49527 to 0.49280, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.49280\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.49280\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.49280\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.49280\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.49280\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.49280\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.49280\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.49280 to 0.49175, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.49175\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.49175\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.49175\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.49175\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.49175 to 0.48979, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.48979\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.48979\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.48979\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.48979 to 0.48968, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.48968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00133: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.48968\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.48968 to 0.48756, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.48756 to 0.48609, saving model to data/best_model.h5\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.48609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00311: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.48609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00476: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.48609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00638: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.48609\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.48609\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=64, activation='relu', input_shape=(len(features.columns),)))\n",
    "network.add(layers.Dense(units=32, activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=500)\n",
    "mc = ModelCheckpoint('data/best_model.h5', monitor='val_loss', mode='min', verbose=2, save_best_only=True)\n",
    "\n",
    "history = network.fit(train_features, train_target, \n",
    "            epochs=1000, verbose=0, batch_size=128, \n",
    "            validation_data=(test_features, test_target), callbacks=[es, mc]) \n",
    "\n",
    "saved_model = load_model('data/best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.772, Test Accuracy: 0.759\n"
     ]
    }
   ],
   "source": [
    "_, train_acc = saved_model.evaluate(train_features, train_target, verbose=0)\n",
    "_, test_acc = saved_model.evaluate(test_features, test_target, verbose=0)\n",
    "\n",
    "print('Train Accuracy: %.3f, Test Accuracy: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph showing train/test loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAFJCAYAAADaAM+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd8VtX9wPHPffbIHoQQRsI6QECGg+FALYjbilpnK9r6q1rbWkeH2qodtrZabetuXXUvxD1w1IEK4kAgcFhhryRkPnn2c39/3AcMZBISMvi+Xy9fkjvOPefJzT3P955lmKaJEEIIIYQQQojuz9bVGRBCCCGEEEII0TYSwAkhhBBCCCFEDyEBnBBCCCGEEEL0EBLACSGEEEIIIUQPIQGcEEIIIYQQQvQQEsAJIYQQQgghRA/h6OoMCLGTUqoQWKK1TunqvAAopWYBvweWATcAP9RaX9qlmWpAKfU/4C6t9fOtHHcT8BNg0x673tRa/7qD83QTkKO1vqIj0xVCiN5MKeUE1gNfa61P6Or8dAal1HHAv4FtwIXAH7XWZ3Rtrr6llHoE6zvIba0cNwv4B1C6x65vtNY/6OA8zQLO1Fqf3JHpip5PAjghmvcD4Dqt9ePJh2j/Ls7PvnhGgiohhOi2ZgJfA4copUZqrZd1dYY6wTnAv7XWf1RKHQ2oLs7PvvhIgirRlSSAEz2CUioduBsYB5jAG1jBVUwpdTNwOhABKoBZWustzW3fI93hyXRTgXysCvRs4M/AYUCRUqoIuARIV0o9rLW+SCl1ClarnAuoB67RWn+abIGaDPQDFmmtL9jjelOAWwE/EAdu1lq/qpTyA/cCw4BsoBY4T2utlVJ9gfuAEUACuE9r/c9kkqcppa4F+gLvAJdorRN7+dn+D/gSOALIAR7TWt+Y3Pdd4Eas7ta1wFVa6wVKKQfwV+BkIAZ8AlyeTHKEUur95Oe5DThnz89dCCHEbi4DngZWAz8HLgVQSl0MXI1VX5QDF2qtNzS1HRiC1StjdPLco3f+vGfdlDz3fiAPq/5YB3xPa709WS/eD/TBqnP+CGwEngIKtdYJpZQPWAsUa63LdhZCKZXXVLrJ/H0XCCbr89OBAqXUW1rrGS3UjbOAHya3V2utj2n4oSmlRmK1hmUDduCfWuuHlFI24A5gElb9bgA/0lrPU0qlAP8CDseqv+YA1yeTnKKU+iSZ/yVY9XCg9V/fbnl6BAhifV/pA7wN/ExrHVVKHQn8DfBhfTe5QWv9ZvK83yQ/pxiwEpiVTDJfKfUaMDC577xeGuCLvSBj4ERP8U+sIGwMcAgwFrhGKTUAuBI4VGt9CNaDcmJz25tI9xLgUa31JGAoUAScpLX+BbAQuFZr/Qfgd1hv3C5SSg0DbgFO1FqPB/4PmJ0MwgAGAeObCN4ygYeB72utJwCnAfcqpQYCJwBVWuvJWuvhwOfAzhaze4AVWusRWBXw/ymlhib3pQJTgJHJNA5v5vM7Wyn19R7/zWiYveS5E5LHnqyUGoEVOJ6htR6b/AxeUkqlYQVrByd/D6OT+Tg7mdZgrC8CI4BK4EfN5EkIIQ54SqlRWM/254BHgR8opbKVUmOxgprjtdYHAS8D1ze3vQ2Xalg3nQN8qrWejPXMrge+nzzuaeA5rXUxcCJWfbcY2AEcnzzmHODdhsFbg+2N0tVa/y2Zzzu01tdi1Qurk8FbS3UjQDFwdBPBmwN4Hvi11vpgYCrW94JJWPV9P2Cy1npU8nPdOWTg94AHq94ch1X3TU3uKwCmAcOxet3MbOazPLKJOvWiBvsnAtOBUcn/fqyUyk7m9+fJ39uFwONKqSKl1KlYAdvkZABeyrffAQYnzxkDfAhc00yexAFEWuBET3ECcLjW2gTCSqn7sAK0v2K9TfxSKfUG8IbW+t3k27dG25tI91fAdKXUL7Ee2P2A1sbgTcdqXXpXqV09QBJYASDAZ1rrWBPnTU6eN6fBeSZwkNb6eaXUGqXUT5PpHA18mjxmGvBLAK11NVbARDKNZ7TWcaBeKbUS621fU1rrQnm/1joKVCmlngNmYL3te1drvSZ57feUUtuxArdpWC11weT5ZyfzdBMwt0GlvqiFPAkhhLBa317VWlcAFUqpUqwXg2HgLa31BgCt9Z0ASqmrmtl+dCvX2VU3aa3/oZQ6MpnWMKx6Zb5SKgvrxdx/ksdtwGrZQyl1N9ZLz9eBHwPX7nmB5tJtJV/N1o3Jf3+jta5p4rzhybw91OA8L1aQeq9S6gaswGkIVp1amzxmGlZvkjhWa9/UZPlmAXO01vXJn5fQfP3VWhfKR7TWdcl0/ovV+rgGWKW1ng+gtV6qlJqXzNt4rKC5MrnvqgZ5WqC1XpVM92uaDyrFAUQCONFT2LAe6A1/dia7ckzFapWbBtyhlHpTa/3L5rbvke5TWH8HzwI7uygYreTFjhXY7GxxItnitxmrW0hdC+ct01pPbHBeP6BMKXUZVoV9F/Ak1pvOouRhsYZlV0oNxuoyAxBtkL7Zhrw3p2HAacOq1Ozs/pnv3OdsIk95fNui31F5EkKIXi3Zc+P7WC8m1yY3p2G1vvyV3Z+zXqxWtD2fvzu37/m8de1xuboG59yKNUzgIeB9rOe6wbd1QcP0FdYEK08AtyiljgFStNYfNlGe5tJtSbN1I3A+Ldep1VrrcQ3OywOqlVInYXWtvB14CVgO7OwVs+fnNwCrpRC6T52aAWR0cJ5ELyJdKEVP8RZwhVLKUEq5sYKducmuJEuwHv5/xurzfmhz25tIdwbwe631M8mfJ2I9ZPcUw3rIArwLHJfsYohS6kTgG6w3fy35DBimlDoqed44rH7uBcl8PKK1fhDQwCkN8vEOcFHynPTk9Ye1cq29dYFSypbsyvI94JXkdWYkA0aUUscCA7Depr4DnKeUcidbO+8Fzu3gPAkhRG93PtbwgH5a60KtdSFWl7kUrC/w05RS+cljf4wV1L3fzPYyYKBSqo9SysDqzticGcCdWuvHgO1YPUvsyZauL7C69+0MbuYB6cmWqcexgrP79ibdJo5rWKe2VDe2RGONqbugQV6XYPUSmQ68orW+F2s4xHfZvU69MFnnubG6NU7dM/F9dHayfvRgfZavYPWqGaGUOiyZ32LgKOB/yTzNTA5RALgJuKqD8yR6EWmBE92NXym159u2ycDPsAYdL8Z6q/gm8CetdUQp9SywMHleEGuw8KKmtjdxveuAF5VSAaAa+IBvu0I29Blwo1JqttZ6plLq/4Cnk5VkDDhVa13XoBtHI1rrMqXUGcDfkg91G1af/7VKqduAB5RSP8R6u/Yp1ng/sN7E3quU+iZ5zp+11l+0dK0mnK2UOmKPbeu11qcm/+0FFmCNZbtnZ3dTpdTlWOP7HFhvKE/RWlcrpe4HCrEqegOrAvon1sQuQggh2uYy4O/J7nwAaK2rlFL/xJok6lrgzeTzfgtwsdZ6s7Imr2pq+/1YAcsW4FWs1rCm/B64TSn1B6wWno/5tu47D7gn2aXfxJr8Y2ty38NYL1D/2450GyoBQkqpBVgvTpurG5u5DCTr/9OAfySHQTiB3yYnKqkAnlJKLcb6rvs2cEbyhePNWK1zi7CCume01rOT49Da6kil1Nd7bIslx9yDVV9+BGRiBYgPJ3sMnQX8S1mTwCSAi7TWK4AVyhoLOS9Z5qVY3VW7zTILonsxTHPP1lwhxIFEtXE9OSGEEAeu5AvLXwGDtNaXdXV+uivVxvXkhNgX0gInhBBCCCFaswarZe+0rs6IEAc6aYETQgghhBBCiB5CJjERQgghhBBCiB5CAjghhBBCCCGE6CEkgBNCCCGEEEKIHqJbTGKSSCTMeHzfxuLZ7Qb7mkZ3JOXqWaRcPYuUa/9zOu3lQG5X56On6Ij6Ebr3PbEvpFw9i5SrZ5Fy7X9trSO7RQAXj5tUVdXvUxoZGb59TqM7knL1LFKunkXKtf/l5qau6+o89CQdUT9C974n9oWUq2eRcvUsUq79r611pHShFEIIIYQQQogeQgI4IYQQQgghhOghJIATQgghhBBCiB6iW4yBE0KIA0U8HqO6upxoNNJp1ygrM0gkunaAttPpIj09B7tdqpmO1p57qDvcE3tL7iEhhGiaPBWFEGI/qq4uJyMjnczMLAzD6JRr2O024vFEp6TdFqZpUlm5g6qqcrKy+nZZPnqr9txDXX1P7C25h4QQonnShVIIIfajaDTSqcFbd2AYBpmZWZ3aynggk3tICCEObBLACSHEftabv3jvdCCUsSsdCJ/vgVBGIYRoD+lCKYQQPcB//vNvPvvsUwzDhmEY/PznV1JcXNxh6S9YsIBnn32G2267vcPSFN2L3ENCCNE7SAAnhBDd3OrVq3j//fd5/PEnMAyD5cuXcd111zF79otdnTXRQ6xaJfeQEEL0Fr0igCurC7M9HKeP297VWRFCiA6XlZXN1q1bmD17NkcccQQjRozk6aef4fPPP+fee+8BIBQKcsstf8HpdHLttVeTl9eXTZs2ccIJJ7Bq1SqWLVvGUUcdxZVX/oJZsy6kqKiI0tJSwGzUYvLWW2/y3/8+is1mZ8KECfziF1d1QalFR8rO3rt76JprrqJv33y5h4QQvVplfYSVZQHqInEOL8rC7Wh5dNnKsjrK6iIcOjADp73rRqL1igDuP5+u5+vNNTxz4cFdnRUhhOhwmZmZ/Otfd/Pkk09w77134/V6+dnPfk5FRQV/+cut9OnThwceuJ+33nqTk08+hQ0bNnD//f8mHA4xY8ZxvPfe+3g8XqZPn8aVV/4CgHHjxnPjjTfx9NNP8cADDzBt2nQAqquruPvuu3jmmefwer38+te/4pNPPmHKlCld+RGIfbS399DGjRt54IH/yD0khGiXDZVBakJRivPTujQfkViCHfURKgIRKuqj7AhE2BaMsWRjFSvLAlQEvp0oKcvn5Kxx/ThzbD8yfM5d203T5JO1lTy+cCML11cBkO5xME3lcsLIPhzUL22/j9ntFQFcPGESiMS6OhtCCNEp1q9fR0qKnz/+8U8ALFmyhMsvv5Srr76GP//5Fnw+H9u2bWP8+PEADBgwgNTUVFwuF9nZ2aSnZwDQsH6ZOHEiAOPGjeO9995rcK31VFZWctlllwJQXx9g48YN+6OYohOtW7d391D//v3lHhJCtMv22jA/evprakIx7j5rDBP6Z+zX62+pCfH0l5t4vWQ7VcFoo/0uh43BWT4mF2YyLNfP0Bw/CdPk6S83c/8n63hkwQZOLs7jrHH9WLKlhie+2ERpRT19Ulz89MgiirJ9vLV8O68u3cYLi7ZQkO7h5OI8Lpo4ELtt/wRyvSKAc9gNovGetUCpEEK0ldYreOaZp7n77ntwu90UFhaSkpLCrbf+hblz38Xv93Pddb/BNHc+B1uvQEpKltK3b1+++uorhg4dumt7QUF/+vbty7///R+cTidz5rzIiBEjOqlkYn/RWvPUU0+1+R5qy9tkuYeEaNr22jAfr6lgQKaXYTkpu7XmdLSNVUEWb6nhONWnw4KHjVVBvthQRZ9MP6OyvaR7257/SCzBr14pIRRNkJ/m5pcvlfDQeeMZmOlt0/n1kTj/W1XOGyXb2VQd5JwJ/Zl5UF8cbeiuWLK1licWbuTdFWVgGBwzNIdhuX6y/U6yfC6y/C6yfU6GFmRQVxtqdP6kwizWVAR4cuEmXl6ylRcWbQFgeK6fm09QTFe5u7pNHjkkm0Akxv9WVvDGsm089vlGjh/Zh/4ZbSvnvuoVAZzTbiPagxYoFUKIvTF9+nTWrFnNueeeg8/nwzQTXH31NXzxxRece+7ZpKWlk52dTVlZWZvTnDNnDv/976N4vV7+/OdbWbFiBQBZWVn84AcXMmvWhSQScfr1K2DGjOM7q2hiPznuuONYtWqV3ENCdLK6cIzLnvuG9ZXBXdty/C6G5voZnuvnlOK+FGb7Wkxjc3WIz9dXcsywHNI8zQdPn5Tu4IbXllMbjvHy4q3cfMII+qS6W0w7nmjc4BGIxFi4oZoF6yqZv66SjVXfBjcGMCIvhUmFmUwclMmY/DRcLYwTu+39VSzZUsutp4xkeJ8ULnrya37x4hIePHccGc0EgrGEyYJ1lbyxbDv/W1lOKJagX5qbbL+Lv723iqe/3MhlRxQxbXhOo5dL5XVh5q+r4qUlW/lqYzV+l51zD+7P2eP70TfN0+T1WgoGB2f7uWHGcC49opC3l29naI6fQwdmNPlSy+9ycFJxHicV52Ga5n7tRml8+8a260SjcbOqqr7d5//jgzU8v2gzH/3siA7MVfeQkeFjXz6b7krK1bNIuTrOtm3rUapzWyPsdhvxFl5qzZp1Ib/73Y0MHjy4U/Oh9XLy8gbuti03N/UL4JBOvXAv0lT92J57qLV7Ym915T3UkDybepbeXq6EafLLl0r4uHQHfz11FG6HjVVlAVaWB1i5vY41FfWYpsmpY/pyyeRB5KbsHmxVBaM8PH89z329mWjcJMvn5OpjhjBd5e4WHJimyeMLN3LXR6UMyfFzcnEe981bi8tu44bjhnP0sJzd0o3EEry5bDtPfrmR1eXNf/4+p52DB6QzcVAmhw3KBKedd5ZsZf66SpZsqSFuWmO/fj51MCcX5zUKWOZ8s4U/zV3JhYcN4IojiwBYtKmay577htH5adx1xpjdgr+EafLmsu3cN28tW2rCpLodTN85rqwgDQOYV7qDuz4qZXV5PaP6pnL54YXETHNXsLmzPH1T3Zx7cAGnju5Lirvl9qnufB+2tY7sJS1w0oVSCCGEEOJAsm5HPe+tLGd4nxQm9E/H62x+NvKEaRKLmy22Hu2rh+ev54PVFVx9zBCOGpINwMRBmbv276iP8NBn63lh0RZeL9nO+QcX8P1DB+CwGTz15SYeXbCBYDTOycV5TFO53PvxWq5/bTmvlWzjV98ZRr90D6FonD++vYK3lpcxbXgOvzte4XXaObwoixteW861L5dwxth8rpw6mFAswexFW3jmq03sqI8yLNfPJZMHYtsj8HLZbRzUL43R+am7tU5lZPgoSnNzyZRB1IVjfLGhmscXbuD3b63g9ZJt/HraMAZlWa2JS7fU8Nf3VjFpUCaXHV64K42xBencOENxw+vLuWXuCm48XgHw6dpK7vqolJVlAUb0SeHKqYM5YnB2o9/PEYOzmVyYxesl27hv3lqueGFxMs8GYwvSOfHIPCYOymRYH3+jcvVmvSOAs9mIJ0wSpnlA/fKEEKI9Hnnk0a7Ogujh5B4SXSkaT/Dogg08PH89keQLfKfdYGy/NA5Lth7F4glWlQdYWWb9t7o8QDAaZ0CG15q4ItfP0JwUhvfxk99MV7u9Ma90B/fPW8cJI/tw9vh+TR6T5XNxzbFDOWdCAfd+vJaH5m/ghUVbcDlslNVFOHJwFpcfWcTQHD8Ahw3M5NmvN3Pvx6Wc/chCLpo4kPdXlqO313H5EYXMOmzArlawQVk+HjpvHPd+vJbHFm7k09IdVNRHCccSTC7M5PxD+nNYM10B2yLF7WDq0GyOHJLFnMVb+deHazjvv19w8aSBnFzcl1++XEKu38UfThrRaCzejJF9WF8V5IFP1uF3OVizo56F66soSPfwp5NGME3ltvj93W4zOGV0X6arXN5bWU6Wz8m4gnQ8LQTsvV2vCOAcduuXbr1ZkQBOCCGEEKK7M02TsroIiT2G89gMg2y/q8lJORZtquZPc1dSWlHPdJXLT44sZENlkM/WVrFgfSX3fLyWez5eu+v4FLedYbkpnDQqjxSPgzXlAZZvr+OdFeW7jhmW6+eEkX04bkQf8loZQ9aUdRUBfvvacobl+rlu+rBWg6T+GV7+dPJILji0P/fPW0c4nuCPJ41oNFuj3WZw7oQCjhmaza3vruLeeWvxu+zc/t1ijky28DXktNv42dTBTByUyd0fl3LwgAzOO6T/roCwI9gMg5kH5XPU4Cxuf38N981bx4OfrcdmGDx4TvPj3H40aSAbKoM8+/VmMrxOrjlmCDPH5u/VWmoep50TR+V1VFF6tF4RwO385UcTCVx03aJ6QgghhBA9WcI0qY/EWx1HtK9M0+T3b63g1aXbmtzvcdgYkmO1lA3L8TMkx89cXcbsb7aQn+bmztNHc/jgLAAK0r1MKrT+XRGI8MWGKrxOO8Ny/eSlupsMqOojcVaXB1iytZa5y8v454el/OvDUg4emMEJI/swtl9ao1Yhmw0yvE58TvuuNIPROD95ZhE2A/52WvFetQqNzEvlzpmjWz2ub5qHv3+3mAXrqijI8LQ60+HEwkwmFma2eMy+yklx8+dTRnLymjzu/2Qt3z90ACovpdnjDcPghuOGM3VoNhMHZXb6/dXb9YpPz5l8QyPj4IQQQnQlpZQNuAcYC4SBH2mtVzXYfw1wLpAAbtFav9glGRWiCWsr6rn+tWWsKAswLNfPxEGZTBqUydiCtA7vrvbKkm28unQbpx/Ul+K+qbvtiyVM1u0IsrI8wP9WlvPS4q0A2Aw4/+D+/PjwQc2Od8v2uzhuRJ9Wr+9z2RnTL40x/dI4d0IBGyqDvLlsO68v28Yf3lrR4rluh41sn5Msv4tQNMGaigD/nDmGfun73hWzOYZhdHpQ1h6HD87aFUi3xuWw8Z3huZ2cowNDrwjgvu1CKUsJCCGE6FLfBTxa68lKqUnA7cBpAEqpDOBnwFDAD3wNSAAnupxpmsxZvJXb31+Nx2HjwsMGsGRLDU9/uYnHF27EZTc4qCCdgnQP2cm1tKx1tZz0S/M028rVnDUVAf763ioOGZjBr74zrMX1y0zTpDwQYWVZgPw0D0WtTMHfXgMyvVwyZRA/mjyQpVtrd1sGYKd4wqQqGKU8EGFHfZQdgQiRWIKbTynulsGV6L16RQDntO3sQiktcEII0Zq//e1Wli4toaKinGAwSP/+A8jKyuTvf7+zxfOWL1/G+++/z2WXXb6fctojHQG8CaC1/kwp1XA66ACwDit482O1wvVIcg/1HtXBKLfMXcl7K8s5bGAGN52gdk1vH4zG+XJjNfPXVvLVxmo+Kq+gKhhlz69bqW7HrnXOhub4GdU3leF9mu5OF4rGue7VZfhddv5wgmp18WnDMMhNcTeacr+zGIbB6Pw0Ruentfmc7jwtveidekUAt7MFTrpQCiFE66699lcAzJnzIqWlpfziF1e16bwRI0YyYsTIzsxab5AGVDf4Oa6UcmitY8mfNwAlgB34c2uJ2e0GGRm7tziUlRnY92LgP4BhsNfntOTXv/4NAC++OJs1a0q5+uqr23RecXExxcXFbb6Ozda4/A3Z7bYW9/dUnV2uQDhGeSDCqm113PRqCeV1YX45Q/HDKYXYGgRUGcBJuamcNL7/rm3xhElVfYTyughldWHW7ahn+dZa9NZaXl26jUAkDsBJY/pyw4kjyWkQeNntNu76ZB2ry+t56MJDGNq/d7RayX3Ys/SGcvWKAG7nJCaxRI99mSmEEF1qwYIF3HHH7TidTs4883t4PG6eeuopzOTscHfccScrV67k2Wef4bbbbufEE49n/PgJrF1bSnZ2Nnfc8Q/s9gN3SucGaoCGA3psDYK3E4B8oCj581tKqXla6wXNJRaPm43e7CcS5l4vyt3RC3k3zItpWvnpjHsokWhc/oZ6a8tHR5ZrY1WQN5Zt57O1lVbXv0CEUOzbe2FAhocHzx3HqL6p1NQ07jbYFDuQ57GT5/ExOsfHScOthaMTpsnm6hBvlGzn4QXr+XBFOT87qohTx/TFZhjM21DNMws3cuFhAxiT03t+d3If9izduVy5uamtH0RvCeBkEhMhRA/04lebeP7LjR2a5pkT+nPmIQPadW44HOGpp54B4IEH7ueee+7F6/Vy8803Mm/ex/Tp8+30zRs3buTBBx8mPz+fCy44nyVLljB27NgOKUMPNw84BXg2OQZucYN9lUAQCGutTaVUFVYjR7u19R4yDGNXINWaMyf05/TxBe3Kj9xD3UNVfZS5K8p4o2Q7i7fUYACj89MYk59Ktt9ljV/zOcn2uxjfygLYe8NmGPTPsMaSTVe53PLOSv40dyWvl2zjwsMGcv3ryzioXxqXThnUIdcT4kDVOwK4nS1wMomJEEK0W1FR4a5/Z2Vlc/31v8Hn81FaWsrYseN2OzYjI5P8/HwA+vbtSzgc3n8Z7d5eBKYrpT4BDOAipdRVwCqt9ctKqWnAZ0qpBPAxMLcL89rhiooKd/1b7qH9b9Gmap78YhMfrK4gnjAZkuPjp0cWMWNk+9Y32xeF2T7u+95BvLJkK//8sJQrX1xCutfJn04agaMDu/MKcSDqFQGcQ1rghBA90OnjC9rd0tEZDMP6UlVbW8s999zF3LnvAnDJJT9q1HqzFxPOHVC01gng0j02L2+w/0bgxo66Xlvvoc7qQrknuYf2v3jC5INV5Ty+cBOLt9SQ5nFwzvgCTiruw7Dc5tfl2h9shsFpY/I5YnA2D89fz0njCuib1nlT7QtxoOgdAdzOSUxkDJwQQuyzlJQUxo8fz1lnnYnX6yUtLY3t27dTUNC/9ZOFQO6h/aEmFOWNku089eUmNlWHKEj3cO2xQzhldN8O6xLZUbL9Lq45dmi3HnskRE9itLVPfGeKRuPmvvxBf7O5hh8+9TX/mDmaKUVtW0ywp+itDzspV88i5eo427atR6kRnXqN/dXa0hqtl5OXN3C3bbm5qV8AhzR9hthTU/Vje+6h7nJP7K2m7qGGDrRnUySW4OPSHbxRso15pTuIxk3G5KdxwaH9mToku9Up+bvagfb76umkXPtfW+vIXtEC59y5kLesAyeEEEKILhJLmLy5bBvrK4PsCESpqLcWfK6sj+Bz2Rma42dYbgrDcv0My/WT6XOxoTLIyrI6VpUHWFkWYE1FPW6HjQyPg6wGE45srwvzji6nNhwjy+fkrHH9OGFkH0bktW3WOiFE79E7AjibTGIihBBCiI63piLAja9rzhrfj1NH923x2Mc+38A9H6/FbkBmMvDK8rsozPJSE4qxaFMNby0v23W8zWDXoth2m0FhlpfRfVNxOO1srQqyujzA5/VV1IRieBw2jhmWw4mj+nDIwMxd4/+F2K8ScbB1ry66B6JOC+CUUl/x7WKmpVrrizrrWrLTvfLLAAAgAElEQVSQtxBCCCE62qJN1Vw1Zyk1oRh/fXcVB+WnUZjd9ALApRX1/PvTdUwbnsOfTh6JrZlZWmpCUVaVB1hVFqCsLkJRto+hOX4Ks3y4HNYL6T27eEWS67bt3C9EV/At/AfeRQ9SfeoTxHLHdHV2DmidEsAppTwAWuujOyP9Pe2ahVImMRFCCCFEB/hgVQXXv7aMvFQ3d5w+mqteXMLv3ljOQ+eOazQNfjxh8oe3VuBz2rnm2KHNBm8AaR4nE/pnMKF/25cAlMANq+XHjIPd1dU5OSA5tizEt+B2ANJfuYCqmS8Szxjcxbk6cHXWE2Es4FNKva2Uei+5mGmn2bkOnLTACSGEEAe2eAeMh5/zzRZ++fJShuT4+c85YzmoXxrXTR/Gsm11PDR/faPjn/16M4u31HDVMUPI9kuA0dEcmxeQ9dgU0t78cVdn5cAUCZD2zs9JpBRQdeYrAKS/dC62us1dnLEDV2d1oawHbgP+AwwD3lBKKa11rKmD7XaDjIymuyS0Rdxh9cV1uh37lE53ZLfbel2ZQMrV00i5Ok5ZmYG9kxexNQw6/RptYbPt27NdiL1RVR/l+teW8fn6KtKSE4Bk+5xk+VzkpLgYnO1jaG4KQ7J9eJqZZj+eMHl4/nru/2QdU4oy+fPJo/C5rGOPHZ7LiaMqeOiz9RxelEVxfhoAG6uC3PNRKYcXZXHCyD5NpmsEd2ALbG1bQQwDUkbv/Qew8/RQFabdBc6O/dszghWY3uwOTbNViRi+hf/At/AfYNhwrduKEarE9GTu33x0pljI+r+j+66PlzLv99hq1lN9+nPE+oyl+pTHSX/xLNJfPp+q01/A9PacGeCNUCVEEnReG9b+0VkB3ApgldbaBFYopSqAfGBDUwfH4+Y+TedZH7Liwpq6cLedFrS9uvNUp/tCytWzSLk6TiJhdvp07q1NGf+3v93K0qUlVFSUEwwG6d9/AFlZmfz973e2Kf1NmzaxcuVKjj766BaPSyQaP9tzc2XGvN5gf91DbaW313HtS0upCEQ49+ACwrEEFQFrBsiSbbWUrY4QTo4jM4ABmV6G5vhxOWxUJmeKrAhEqApGSZhwUnEeN0wf1qir5LXHDuXLDdX87g3N49+fgMdh45a5K7HbDH4zfRhGw66T0SDu0jdx69m4NnyIYcbbXB7Tk0HKkJMJDZ9JLP8QMFr5shkN4l47F7d+AdeGD4hlj6TqjJc6prthLEjKvD/iXfIodUfcTHDsD/c9zTaw1Wwk7Z2f4tzyOSF1JqERZ5Hx0tm41r1LWJ25X/KwP6S//kPslSupmvkiidSCrs5OI6617+AteYL68ZcR7Wd1qIvljqHmpIdJf+UC0l/9PtWnPYPp6tpF41sUCeAufRPPitk4N3wEdhephccRVmcQGXAU2J1dncO91lkB3MXAGOBypVQ/IA3Y0knX+nYZAelCKYQQrbr22l8BMGfOi5SWlvKLX1y1V+d/+umnbN68qcO+fIuepzvdQ+/oMm5+U5PmcfDAOeMo7tv4JUHCNNlcHWJlmTV5yMryACvL6ognTLL8LvLTPBT3TSXL76Ioy8eMEbm7B2NJKW4HNx6vuOy5b/jPe19xiK+cxIYN/OnQARTUfQN1YAtX4171Kq41b2CLBoinFBAcfxnRPmOwwseWGfEIKVs+xLP8BbxLHyeeOoDQ8NOJDjgS07b71zZbuAb36tdwrX4dW7SOeEo+4WGn4dEv4Pv8Duon/ardnyuAvWIZaW9fgWOHJp42CP+ntxDpfwTxbLXXadlqNmDUVuKoC7d6rKNyNf55vwdMaqb/i/Dw08FMEPfn4S59u9MDOCNcjRELkvC3POsogK1mPfibX6uwJY6tX+La8AEA6S+fR9XM2fu/lbMlgXJS37uWWPZIAhOv2W1XtGAyNTPuJe2NS0h740dUn/wo2N2tJmmrWU8irX2f115JxHBu/BiPno17zZsYsXriqf2pn/ATPARxLX0Bz6qXSXiyCA871XpZkjfeagHfW6aJrW4LidR+HV+OZnRWAPcg8IhS6mPABC5urvtkR9j5hkwmMRFCiPa7/fbb+Prrr0kk4syadTHTp0/n8ccf47XXXsVms3HIIYdyxRVX8MgjDxEOhxk7dhxTp07t6myLbqQz7qH56yr5cFUFAzO9DM31MzTHT7rXScI0+fvcFdz74RoO6pfGraeOIqeZ8Wc2w6B/hpf+GV6OGZbT/gLGghwe/og3ch9j6Mr5OI04p7qBb5L/JSVcaYSHnUpYnUE0/7DWW9D24D3sPKom/2FXq4Hvy7swvvhnk8cmXKmEh55kXavfJDBsmIYD35d3Exn0HasFb2+ZJp7Fj5DyyR8xXWlUnfIEsZxRZD09jdR3fk7VmS+3vXUvEcf35d34FtyOYcZpa+fHaN4Eao6769sv+4aNSOFxePQLVrfDTuxymP7aLJxbPifSbxJhdQbhISdiutN37bcFtuFe+TLuFbNxli0moU6Gafft9XV8X91Lwp1OzfS7SH/zEtJfuYDq7z6L6eoGPRVME/sbV2GEq6k57ckmg7NI0XHUHns7ae9eie+rB6g/5KctJula+y7pr11I1ekvEO03sVPy7Chfglu/gGfFS9iCZSTc6YSGn05InbGrNduV4aPq0OtxbfgAt56Np+QpvIsfIZZeSHj4TELDTyeRUdSmSxqhKlL+92s8q1+l4vyP2nzevuqUAE5rHQHO64y0m5JsgJNJTIQQPYrxzdPYFj3RoWkmxp4P4/f+8fu//73P9u3beOyxxwmFQpx77jlMmTKFOXNe5Kabfk9xcTHPPPM0NpudWbMuZvPmTRK8dQNtvYcMA+xtrCITY8/HPOicvc5LR99D8YRJVTDKFS8sxmk3dqvj+6S4GOvcyOeVXk4bM4xfHju0Q2ZqNEJVOLZ/jWHu8UI4Hsa19h3cq1/HFqnF78vjedcpvBNS/Gr6CHJSGny5tTmJ5o3f9wDD5beCB3UGRmA7jh3LGx9jcySv5d1tc+DIm3Bt+oS0d37OjrPfBpe/yUvYK5Zhr9ujg5Rp4ln6OO61cwkPOpbaY/+O6bOC3tqj/0r6Gz9sc+uerXYzqe/8DNfmzwgNOw3HId+nLhBpvex2F9G+h8AeLY7houPwLn0M18Z5RAq/03o67WAvL8G55XPCg47FXr2W1PevJeXDG4gUTiOafxiude/h3PgRhpkg2mcs4aIZuPWrOEfN36ugxF65GteaN6k/5GdEBx1DzYz7SXvjh6S9fjHVJz/WtWPiTBNPyRPY9KvUTbmBePbIZg8NjziTyPLn8JQ8Sf3BP2nxZYVnyaMAuNbObXcAZy8vwb7neFLTxFFegnvFbByVKzFtLiKF3yE0/HTrPmmqZdDuJFI4jUjhNOoitbhWv45Hz8b3+R34P/870bwJhNQZhIee0uwYP+fm+aTO/Sm2+u3UTb6ORHphu8rUHr1iIW/DMBo93IUQQrTdihUrWLJkCbNmXQhAPB5j8+bN3HLLX3jkkYfZvHkT48aNxzTlOSua1pH3UHUoyrbaMKFogosnDeTiiQOpDUVZmVw/LXPNHC4qv5W410EsegyhtWcQKZzWvi+98TCute/iWTEb19r3MBJNBxgJp5/IkJMIDZ9JtGAyE0IJioJR0rN9RPf+qnvF9Pch6m96gpQmj3elUjvtTtJfPJOUeTdTd8xfdz8gHsb/6a34Fj3Q9Pk2lzXe7aCLd+tSFhk8g+DIs5Ote8cSyz+02Ty4Vr9O6vvXQiJGzXfuJKzOICPTT3Qfxh1H+08h4UzBVfp2pwVw3pInMW0uaqf9A9OdgWP7ItwrZuNZ+RLu1a8lu+FdQVjNJJ45FKJBcp48Cv+831szNLaxtdX79f1gdxEcYy2THCn8DrXH/p20d35G2luXU3PCA40C2M5mq1mPZ8WcZCC0isTAwwmOvaTV80LF55H29k9wbviI6MCmX8rYajfhWvc+AK71HxCYcsNe58+9/HnS3r2y2f2R/InUHv0XwkNO2quJbkxXKuGRZxMeeTa22s24V87Bo18g9cPrSfn4RiIDjyE8fCbhomnWy5JEDN/nd+D74l/E0wZSNXMOsbxxe12efdErAjgAl91GTLpQCiF6EPOgc4i3o6WjMxQVDWbSpMn89re/Ix6Pc++991BQUMCdd97BTTfdjMvl4oc/vIhvvvkGm80gIc/bbqGt91BrE9t0hKKiwUw67FB++7ubiCfMXffQHXfewbXX/Rabw8lPL72ED+d/QU0kRnUwwpaaEHabgSP5n80w2FEfJRCJ4XXayUlxcdmwQQC4U9zkpLiZan5B2ue3ESmYjL3/BByLnyd97Vyr2+KQE4n2PxyMpmeZ3I0Zx7npM9yrX8UWribhzSU45kIihdMw92jRwjCIZY0A57fbM3x2Mnzdd/KDaL+JBMdfiu+re4kUTidSNB2wWn5S3/4JzvIlBEdfSEjNZM+xeYmUfBIp+U2mGzjiZlybPiXtnSupPPutRpNXGPXl+Of/FW/Jk0T7jKVm+l0d163M7iYy8Ghca+eC+ee97praqmgQ94oXrS6TyQAgljeOWN44AlN+i712A/H0wt2v6/QSP/p6nK9egXvVK4SHndbqZWyBbXiWP09o1Dm7WjcBwmomteFqUj/6Lanv/JxI0YxG50YKJmP6clu9hmPrFyRSC1odx2eEKnGves2a4GPLAusa/SZSO/YSPIeeC/Wtf8bhwceT8GTiLXmi2QDOU/IUAMHRP8C75L/YAttI+PNaTXsnV+nbpL53NZGCwwlM+iWN7ll/3w4Zg5ZI7UdwwuUEJ1yOvbwEj34B98o5uNfOtborDzkRR+UqnFu/IDTiLOqO/EOXTODSawI4p90mk5gIIUQ7TZs2jYULP+cHP7iA+vp6pk8/Dp/Px5AhQzj77LPIzMyib9++jB49GpfLxUMPPcjIkSOZMeP4rs666CamT53Mlx+9wYXnf4/acJwjjp7G1iCk9OnPeeeeTVp6Bjm5eeQOHEbEtPHMY48wcPBwJh71HUy+rb/thkF+mocMrxNdtvuXNOfmz0h761JiuaOpOfFh0vv0oWrCL3FumodHz8az8mW8y55uc55Nh5fw4BMIqZlE+x+x31s8Oltg4jW41v+P1PevZUffd3GXvkXKRzdiOjxUn/gQkaLj9jpN05VC7XfuIP3FM/HP+73VurfHjJuYCeon/ITAYVd3+MLbkaLpeFa/imPbV8T6HtyhabtXv4YtXE2ouIlu6HZnswtXm2POJvbpPfg//TPhohmttgR7v3kIzBj1TbRuhQ66CFu4Cv+C2/GsfKnR/nDhdGpOerjF9O3lJWS+cBqmYSPa/whCw2cSGXz8t4FGE63Oscxh1E36NeFh3yWR1h8Aj8sH9W1oMbW7Camz8C5+CKO+rHGAmYjhWf4M0YFTCY46H++S/+Lc8CHhEWe1njbg3PQpaW9dlvy7f3C/BUzxnFEEckYRmHwdzk2f4lkxG/eqV8GwUTP9LsLDv7tf8tEUozt0h4lG4+a+TuV94v3zOWJwJtdNH95BueoeZPr2nkXK1bN0Rbm2bVuPUiM69Rr7o7WlLbReTl7e7rON5eamfgG0Y1aFA1NT9WN77qGOvidiCZNQNE4sYRJLmMQTCXLC63GaEWxmglp8rCcPr9OB32XH67TjsBnY7QZ2wwAzgRGts1q6bE7ipkk8bhIzTdx2G3abFbg1vIccZUtIn3MWCX8eVafPxvRmNf4bjgax125sczniqQUdvl5aR+jIZ5O9YhmZz56E6U7HFiwj0v8Iaqfd2aYZFlvi//QWfF/eQ7hwOs5Nn+yacTM8/HRCaibxrMbfxzqiXEaokuyHxhEcfymByb/Zp7T2lDF7JkZ9GZXnf7hXsxFmZPgILH6LjJfPpW7KDQTHX9rssUaklqxHJxIZcBS1xzc/8YmtZiNGLLjbNu+SR/Es+S87fvAZiZTmW5tSPrgez7KnCY69BPfKl7DXbsB0eAgXzcB0+ncFqglvLqHhpxFWZxDLGd2ozHvz+7JXriLryaOpm/wbghN+sts+19p3SH9tFtUn/JtI0QyyHz6YSP8p1B53d6vpOsoWk/7iWSRS+u76u99X+3Qf7vyd7NlK30HaWkf2mldNMgZOCCGE6BymaVIfjVMXjhOIxAnHdl/TLNMI4DFCbLP1wW03yIhuY6S7kkTaAPbs6mTEQthr1mPEQ5hYLTo2dyZ2dzquZro+2qvWkP7KBZiuVKpPebL5L3FOL/GsYR1Q4t4jnj2SwJTr8H/yJ+omX2cFFx3Q9TBw2NU4N3yMc/N8a8bN4TOtiSk6ulvjHkxPJtF+E3GVvt2hAZx9xwqcWxZQN/n6dk0lHx1wJOFBx+Jb+E9CI89udgyWZ+kT2CI1BCdc1mJ6O1vBGqof+yO8ix/Bs+xZ6g9tZixYNIh7xWzCQ04iMPnXBCb9CsfWhdZ0+qtexohHkq3OZ1jdjTuo1TmeOZRI/kQ8JU8RHH/ZbveBZ+mTJLy5RAZNs2YTHXgUrnXvQSIOtua7O+/6u3entfx3vz91UuC2t3pNAOew24h2gzfOQgghRE+UME0CkTixeKJBy5r1/1AsgWmaGIaBz2knPcWNz2VPjl0D545NmIaHrKw8wCBeb1gzxdU5dmspsAUrsNVtBsNOPHUAxMPYQpXYIhuw1W7CdKdZrXLJ789GpA7vV/fjXfwwYFJ96lP7da2l3iI49kcEiy/o2JkN7W6qzpiT/HfHdpNsTaToOFI+vgl71ZpmuzXuLU/JU5g2J6E2dutrSmDy9WQ+Mx3f53cSOPLmxgfEw3gX/ZtIweHE+ozd6/QT6YVE+h+Jp+Qp6g/+aZPBj3vVK9gitd92AzUMYvmHUpd/KHVH/h5ItGm9tvYIFZ9H2js/x7npUys4BGx1W3Cte4fg+Mt3LZgdGTAVj34BR9niZif/sNVtIf2lcwHk774JnfuaZD9y2g1iCWmBE0IIIdrKxKQuHGNTcpHrjVVBttaGqQhEqAvHiMZNbIZBltfJwEwvKtfPwEwv2X4XXqcdp92GPVSBkYgQT8lnZ+SV8PUh7s3BHizHVr8NEjHs1Wux123CdKUQyxpOwpNJwt+XWPYIYhlDMD0ZGJFa7IEt2Ous/2zhKlI++QNGNED1KY8TzxzStR9YT9YZ09LbXfs9eAOscWZYE1t0iFgIz/LnrC6GvvavExjPVoRGnot3yaPYq9Y02u9eMQd7YBv1Ey5v9zVCo87DXrcJ54YPm9zvLXmSWMYQovlNTNNvd3Za8AYQHnIiCXc6npInd23zLH8Ww0wQHPXtZEuRgVMxMXYtYt4U//y/YQtVyN99M3pNC5zTbpMulEKIHmFnS0Zv1h3GV/dm+3oPReIJdtRHqQlFiSdM7DaDDK+TNI8DV4MxaK1nJIYtsJ2EK7XR4sOJlHwMM449sA1bfTmYCeIp/Uh4s9m9W6WB6fQTd/ohtQCSa7CZpkl8h5PyS5Zh2t1dEiiI7imRNoBY9kjcpW83Hm9mJnDrF7DXrG90Xjy1P2F1ZqOWK/eaN7GFqwgVn7/PeQscdjWeFS+S+u5VRAYcuds+j55NNKeY6ICj2p1+ePAMEt5sa8bHQcfsts9eoXFuXUjdlN+2qxvoPnN4Cakz8C55nLrgDkxPBp6Sp4j0P3K3NdJMbzax3DG41n9A/SE/b5SMrW4z7hUvEiy+gFjumP1YgJ6j1wRwsoyAEKIncDpdVFbuIDMzq9cGcaZpUlm5A6dTvnB3ho64h8pqw9SG46R6HKR7HPjddgz2Pi1bYDuYcRL+pqacN4in9rcmLImFiacNaDw9fxPnYNite6hqB06nu1FgKARYi3r7vvgXRrAC05sNWNPzp75zJa6NHzV7XkS/QO20f+y2TIKn5AniaQN3dfvbF6a/D3VTriPloxtxbl24+z7DTt3x9+9bcGV3EVJn4v3mwUZT8XuSa9jtSzfQfRUadR6+bx7Co58nljUce+1GApOvb3RcZOBUfF/egxGuwXSn7bbPu+hBMBMEx/3f/sp2j9NrAjiZxEQI0ROkp+dQVVVOWVlZp13DWqeta5+HTqeL9PT2d0USzWvPPbTnPVEbCOAywwScdgINDzQMTGdK2yaiSMSwB7ZhOn0kdqxt/fiydW3OL8g9JFoWKZqBf+E/cK19h/DIs3GVziX1vasxYvXUHn0roVHn7R4omSbu5c+S+uFvyXx6OrXH3kZk8PHYq9bg2vQpdZN+3WETsITGzCI0ZlaHpNVk+sXn4/v6ftzLnyN48BXWxlgIj36e8ODju3Syj3j2CKJ9D8az9AniWcNIeLMJD268nl104FSML/6Fc9M8IoNP2LXdCFXhWfo44aGnJCdBEk3pRQGcjXAk1tXZEEKIFtntDrKy9m367tb01mUfhKU999Ce90T9v89kfGQ+OP27HWfE6jE92dRMu4PowKNbTDPtzR/jWvceOy74eK8W5BWiI8RyxxBPycez6hWcZYvxLn6EaE4xtcfdTTxzaOMTDIPwyLOJ5R9K6ts/If2NHxEs/j7Y7Jg2B6ER39v/hWineMZgIv0m4S15iuCEy8Gw4V79enINu33vBrqvgqPOJ+29q3BUraZ+/KVNdn+O5h1MwpmCa/0HuwVwnqWPY4sGqB/f8iydB7peE8A57AZ1MomJEEII0SpHIswqx3Ay/+/d3bbbK5aR9vYVZLxyAfVj/4/A5F81mvTAiNTi1i/gXv0agUOvkuBNdA3DIFJ4HN4ljwJQP/YSApN/3eokHfGMwVSd8RL++X/F95W1Dlt48PGY/j6dnuWOFCo+n7S5P8W5cR7RAUfiKXmCWHoh0YLJXZ01wkNPJvHxjdZsmKOaWBQdwO4k2v9wXOs/ANO0WktjIXyLHiQycCrx3OL9m+keptcEcDKJiRBCCNE2djNGvIm34vHskVSe9Sopn/wR36IHcG76xGrRSBuIa8MHuPVs3KVvYcTDRHNGUz/ux12QeyEsweLzcZR9Q+DQqxpN6NEiu4vAlBuIDDgK//y/UT++/bNCdpXw4BNIuDPwlDxJIqUfrs3zqZv8m05fh69NnD7qD/4p9uq1LS7zEBk4FXfpW9irS4lnDMajn8cWLOuRv4/9rZcFcDKJiRBCCNEauxkhYfia3unwUnfUn4gMmErqe1eT+ezxmA4vttAOEp5MQqPOITR8JrG8CV0z050QSfGcUVSd+Uq7z48OOIqqfZgRsks5PIRGnIl38aNgd3e7bqDBNiyVEBkwFQDXuvcJpg3C+9V9RPuMJVowpbOz1+P1ogBO1oETQggh2sJpRgnbWp4lNFJ0HJXnzMU/7w9gmoTVTOsLV3IxXiFE1wqNOg/fov9Yk5cMORHTl9vVWdorifRBxNILcW74gHhKXxzVa6mecZ+8GGqDXhPAuew2YtICJ4QQQrTKbsZI2FoPxBL+vtQed/d+yJEQYm/Fs4YTzT8U55bPCY7q+slL2iM68Gg8y57BVl9GPG3QbhOaiOZ1g46yHcNptxGVFjghhBCiVU6iJFppgRNCdH+Bw64hNOJ7RPdYNLyniAw8GiMWxFm22Jqxco9F1kXTek0LnEPWgRNCCCHaxNHGFjghRPcW7X94hyxA3lUi/SZj2pyY7nRCI87s6uz0GL0mgJNJTIQQQoi2cRLFlBY4IURXc/kJTLyWeNpAcHi7Ojc9Rq8K4GQSEyGEEKJlpmniIorZxDICQgixv7Vlxkqxu140Bs6QSUyEEEKIVsQSJk5iIAGcEEL0SL0ogLMRNyEurXBCCCFEsyLxBC5imDIGTggheqReE8C57FZRpBulEEII0bxINIbTiIPd3dVZEUII0Q69Zgycw24t+heNJ3A7ek1cKoQQogdRStmAe4CxQBj4kdZ6VXLfOODOBodPAr6rtX5zf+YxGo1Y/3BIF0ohhOiJek0A59zZAidLCQghhOg63wU8WuvJSqlJwO3AaQBa66+BowGUUmcBm/d38AYQi4QAMOzShVIIIXqiXtNU5dzZApeQiUyEEEJ0mSOANwG01p8Bh+x5gFLKD9wM/Gz/Zs0Si+4M4KQLpRBC9ES9rwVOxsAJIYToOmlAdYOf40oph9Y61mDbD4HntNblrSVmtxtkZPj2OVN2u21XOtvLrBeebp+vQ9LuSg3L1ZtIuXoWKVfP0hvK1esCuKh0oRRCCNF1aoDUBj/b9gjeAM4HzmxLYvG4SVVV/T5nKiPDtyudmkorvowm7B2SdldqWK7eRMrVs0i5epbuXK7c3NTWD6IXdaF07QrgpAulEEKILjMPOBEgOQZuccOdSql0wK213tAFeQMgFgsDYHNIF0ohhOiJek0LnMNmdQmRSUyEEEJ0oReB6UqpTwADuEgpdRWwSmv9MjAcWNuF+SMR3RnAySyUQgjRE/WKAM65+TMGbVgBFMokJkIIIbqM1joBXLrH5uUN9n+ONVNll4lHpQVOCCF6sl7RhdKz7FmGlPwLkDFwQgghREsSMWsdOJvT08U5EUII0R69IoAz7S5sCatCikkLnBBCCNGsRHIMnN0h68AJIURP1EsCODe2uFUhSQucEEII0TxzZwDnkhY4IYToiXpFAIfDvasFTgI4IYQQonmJuFVf2qULpRBC9Ei9IoCzWuAigCldKIUQQoiWJMfAOZwyC6UQQvREvSaAA3ATlRY4IYQQogU7u1A6pAVOCCF6pF4RwLFbACctcEIIIUSz4lEAHE5ZRkAIIXqiXhHAmcm1bFzEiCWkBU4IIYRoVnLSL5sEcEII0SP1jgBuVwtcRLpQCiGEEC1JTmKCXcbACSFET+TorISVUn2AL4DpWuvlnXUdYFcl5DaiMomJEEII0ZJkAGfaZB04IYToiTqlBU4p5QTuB4Kdkf6eZBITIYQQom0MaYETQogerbO6UN4G3Ads7qT0d2ffOQZOJjERQgghWmIkokRxgNErRlEIIcQBp8O7UCqlZgFlWuu3lFK/acs5drtBRoav3dc0MtIB8BhR7E7HPta5wzAAACAASURBVKXV3djttl5Vnp2kXD2LlKtn6a3lEh3DlohYAZwQQogeqTOe4BcDplJqGjAO+K9S6lSt9dbmTojHTaqq6tt9QUfQJBPw2WLU1Uf2Ka3uJuP/27vz8Mqu+sz33z2cQfORq1TzYIztBTbYYGxssAEz2EDSBIdO0t0ECAZurvvCTXi43UDngQz90M1z0yG3gTAnJGQg4ADuuGnANDMYTJgMtrFXeR5rLmuWzp7W/WPvo5LtUpWqNBztrffzPHp0pHO0z1qSSqve81tDq7dS/elQv8pF/SqXtdyvkZGBbjdh3fOyiAStfxMRKatlD3DW2ud3bhtjvgVcc7zwtiyKKZR9fqJNTERERI7Dz2JiTwFORKSsKjEBvrOJSdNPtImJiIjIcfhZTOppCqWISFmt6F9wa+3lK3n9js5B3j1ewoQ2MREREVlQkEUkXk2TKEVESqoSFbjOVsi9fkycqQInIiKyEN/FJJpCKSJSWpUIcC5oAtDjxSSqwImIiCwodDGpApyISGlVJMAVa+C8hEQVOBERkQWFLib1FeBERMqqEgGO8GiA0yYmIiIiCwuymEwVOBGR0qpGgPNDnOfT9GJiTaEUERFZUEhM5te73QwRETlF1QhwAGGThqdNTERERI6n5hTgRETKrEIBrkETbWIiIiJyPCEJmdbAiYiUVnUCXNCkrjVwIiIiC3LOUXMxLlAFTkSkrKoT4MIGDSLtQikiIrKAOHXUvASnKZQiIqVVoQBXp4E2MREREVlIlGbUSXCBplCKiJRVdQJc0KRBrAqciIjIAuI0o06sCpyISIlVJsC5sEFNFTgREZEFRamjTjJ3fqqIiJRPZQIcYZOai7SJiYiIyALiJM0DnKZQioiUVnUCXFCnrimUIiIiC4qTCN9zeIEqcCIiZVWdADdXgdMUShERkWNJoii/oWMERERKq0IBrkHNqQInIiKykCSeBcALFeBERMqqUgEuVAVORERkQWncBsDXJiYiIqUVdrsBy8UFDWouInOQZo7A97rdJBERWWeMMT7wYeB8oA28yVp717z7Xw78UfHhT4E3W2tXbepIMhfgVIETESmrylXgAFXhRESkW64Cmtba5wDvBN7XucMYMwD8N+BfWWsvAe4DNq5m4zoVOE8VOBGR0qpQgGsSZDGA1sGJiEi3XAZ8BcBaexNw4bz7ngvcArzPGPNdYL+19uBqNi5LNIVSRKTsKhTgOhU4R6Kz4EREpDsGgbF5H6fGmM5yhY3AC4F3AC8H3mqMOXs1G5cm+UwVTaEUESmvyqyBozjTpkFMnGkKpYiIdMU4MDDvY99amxS3DwM/stbuAzDGfAd4BrBnoYsFgUer1bvkRgWBT6vVSyPIx8eB1uCyXLfbOv2qGvWrXNSvcqlCv6oT4MJ5AU4VOBER6Y4bgVcA1xpjLiGfMtnxE+BpxpiNwChwCfCJ410sTR2jo9NLblSr1cvo6DTTU1MARJG3LNfttk6/qkb9Khf1q1zWcr9GRgZO/CAqFeCaANRJtImJiIh0y3XAFcaY7wMecLUx5m3AXdba640x/wm4oXjstdbaW1ezca5YAxfUtAZORKSsKhPg3FwFLiLWJiYiItIF1toMuOZxn75j3v2fAT6zqo2aRwFORKT8qrOJSWcNnBeTagqliIjIEyX5bs1hXQFORKSsqhPgQm1iIiIicjxZqmMERETKrkIBLl8Dp01MREREjs0rApzzdYyAiEhZVSjA5a8m1om1iYmIiMixpPkUSoJad9shIiKnrDoBLshfTWx4sTYxEREROQYvzQ/ydoEqcCIiZVWZAOfmTaFMVIETERF5orkApzVwIiJlVZkAN38Tk0QVOBERkSfwsjzA4WsKpYhIWVUowHUO8tYmJiIiIsfiZzEZHviVOQZWRGTdWdRfcGPM84Fe8sD3QeDd1tpPr2TDTtq8c+C0iYmIiCyHUox/J8HLImJq4HndboqIiJyixVbg/hS4E/g94FLgmhVr0al6zDlwqsCJiMiyWPvj30nw05jEU/VNRKTMFhvgZoD9QGKt3QesvdXPcwEu0iYmIiKyXNb++HcS/CwiQevfRETKbLEBbhz4GnCtMebNwAMr16RTNLcGLtEaOBERWS5rf/w7Cb6LSTwFOBGRMlvsPIrfAp5srf2lMeZc4BMr2KZT44c4L6DhxbQ1hVJERJbH2h//TkKQxaReDa2AExEpr8UGuKcDrzfG9M773BtWoD1L4oI6jThmUlMoRURkeZRi/Fus0EUkQU2TKEVESmyxAe4jwF8A+1awLUsXNGh6kTYxERGR5VKO8W+RfBeTenUFOBGREltsgBu31n5qRVuyDFzYoOml2sRERESWSynGv8UKXUymM+BERErtuH/FjTFXFjfHjDF/APwEcADW2q8e5+sC8nUCBkiBq621dy9Li48naNL0dJC3iIgszamOf2tdzcVkfr3bzRARkSU40ctw/654PwacVbxBPogdbwB7BYC19lJjzOXAnwOvPPVmLo4L6kWAUwVORESW5FTHvzUtcAmZ19PtZoiIyBIcN8BZa68GMMZsBJ5prf3fxpi3AH9/gq/7H8aYLxYf7iY/Q2fFuaBBk5hEa+BERGQJTnX8W8ucc9SIyQJV4EREymyxE+H/EfhYcfsI+QD2r473BdbaxBjzKeDXgd843mODwKPV6j3eQ04oCHxo9ND0I7zQX/L11oogqE5f5lO/ykX9Kpeq9qtLTnr8W6vi1FEn0RRKEZGSW2yA67PWfg7AWvtpY8z/sZgvstb+jjHmHcAPjTHnWGunjvW4NHWMjk4vsinH1mr14qjRYIrpmXjJ11srWq3eyvRlPvWrXNSvclnL/RoZGeh2E07WKY1/a1GUZtRIiH3tQSkiUmaLDXCRMeYK4Cbg2eQbkyzIGPNaYIe19r3ANJCd6GuWRVCngTYxERGRZXNS499aFqcZvV5MrCmUIiKlttgA9ybgz4D3A7cD/+cJHv8F4K+NMd8BasBbrbWzp9zKRXJBgzqRNjEREZHlcrLj35oVpY4hEpwCnIhIqS0qwFlr7zLGvAs4B9hzoiMBiqmSv7UM7TspLmzmFThtYiIiIsvgZMe/tSxOM+okoDVwIiKl5i/mQcaY3yM/1+25wMeNMf9hRVt1qoIGNe1CKSIiy6Q0498iRGlGnRhCBTgRkTJbVIAjPw/nedbatwKXAv9m5Zp06lxQp+5iEk2hFBGR5VGK8W8x4sRRIwFNoRQRKbXFBjjPWpsAWGtjIF65Jp26o2vgVIETEZFlUYrxbzGiJKHupXgKcCIipbbYTUy+Z4z5HPBd4HnAjSvXpCUIG9RcrE1MRERkuZRj/FuEJG4D4IWNLrdERESWYlEVOGvtfwD+mjzwfdJa+x9XtFWnyBVr4BTgRERkOZRl/FuMuQCnCpyISKktdhOTTcCVwBXAi4wxwyvaqlPkgvxVRT9td7klIiJSBWUZ/xZDFTgRkWpY7Bq4z5Kff/MO4B7g71asRUvRCXBZ1OWGiIhIRZRj/FuErAhwvgKciEipLXYNHNbajxY3f26MWfUz3hbDharAiYjI8irD+LcYaVIEuJqmUIqIlNliA9wdxpjXAN8AngUcNsacDWCt3bNSjTtZnSmUnipwIiKyPEox/i2GKwJcoAqciEipLTbAPQUwwBuLj5vAxwAHvGgF2nVqigAXKMCJiMjyKMf4twhpnI+Nfk0BTkSkzI67Bs4Y81kAa+0LgS9Za19Y3J4tbq+pwcsVO2sFmkIpIiJLULbxbzFcqgqciEgVnKgCt2ne7V8B/qy4vTZPylYFTkRElscpjX/GGB/4MHA+0AbeZK29a979HwAuBSaKT73SWju2XI0+ns4mJoEqcCIipbboTUwAb8VasUxc2ASgRkyaOQJ/zTdZRETWvpMZTK4Cmtba5xhjLgHeB7xy3v0XAC+11h5azgYuhkvzFzfDeoN0tZ9cRESWzYmOEXAL3F6TOpuYNDwd5i0iIktyquPfZcBXAKy1NwEXdu4oqnNnAR83xtxojHnDcjR0sVySBzgd5C0iUm4nqsCda4z5NPmrj/Nvn7PiLTsVxaDUICbJ1nzeFBGRtetUx79BYP6UyNQYE1prE6AP+CDw50AAfNMY82Nr7S8WulgQeLRavUvpR3Edn5qfANDfGoJluOZaEAT+snx/1hr1q1zUr3KpQr9OFODmn3fz0QVurxlzFThUgRMRkSU51fFvHBiY97FfhDeAaeD91tppAGPMN8jXyi0Y4NLUMTo6vehGL6TV6iWamQFgYjojXYZrrgWtVu+yfH/WGvWrXNSvclnL/RoZGTjxgzhBgLPWfntZWrNKOgd514mJU1XgRETk1Cxh/LsReAVwbbEG7pZ5950NfMYYcwH5EobLgE8tqaEnwUtjAFxQW62nFBGRFXAym5isffPXwGWqwImIyKq7DrjCGPN98imXVxtj3gbcZa293hjzD8BNQAz8rbX2tlVrWeeIHa2BExEptUoFuPlTKBNV4EREZJVZazPgmsd9+o559/8p8Ker2qiClxUVOF8BTkSkzE60C2WpHA1wEbE2MREREZnjFccIqAInIlJulQpwzK2BS0i0iYmIiMgcL8sDnFOAExEptWoFOD8k84LiHDhV4ERERDo6UyhVgRMRKbdqBTgg8+v5MQLaxERERGSOn0UkBOBVbugXEVlXKvdXPA9wkSpwIiIi8/hZTOLpCAERkbKrXIBzQSPfhVKbmIiIiMzxs0gBTkSkAioX4LKgQd3TJiYiIiLzhS4mVYATESm9ygU4F2gKpYiIyONpCqWISDVULsBRTKHUJiYiIiJHhS4mU4ATESm9ygW4zho4VeBERESOClxE6ivAiYiUXeUCHKHWwImIiDxe6BJSX2fAiYiUXfUCXNCkQaRdKEVERArOOU2hFBGpiOoFuLCuKZQiIiLzRKmj7iVkqsCJiJRe5QKcFzaLAKcplCIiIgBRklEnwQWqwImIlF3lAlxnDVysKZQiIiIARGlGnRinCpyISOlVLsB5YZMmkTYxERERKRytwCnAiYiUXeUCXH6Qt9bAiYiIdERpRo0Ep2MERERKr3IBjjA/B067UIqIiOSiJKPuxaAKnIhI6VUuwLmgWAOXpN1uioiIyJoQJSk1Egga3W6KiIgsUSUDHIBLoy63REREZG2IkowGCYSaQikiUnaVC3CdVxe9ZLbLDREREVkbOrtQqgInIlJ+4XJf0BhTAz4JnA40gPdYa69f7udZiAuLwSltr9ZTioiIrGlRkm9i4mkNnIhI6a1EBe41wGFr7fOAlwN/sQLPsaDOFEo0hVJERASAOIoJPIcfKsCJiJTdslfggH8CPjfv42QFnmNhnSmUqaZQioiIACRxPiZ6oaZQioiU3bIHOGvtJIAxZoA8yL3rRF8TBB6tVu+SnjcIfFqtXrzBAQDqJEu+5lrQ6VfVqF/lon6VS1X7JacuifNlBarAiYiU30pU4DDG7ASuAz5srf30iR6fpo7R0eklPWer1cvo6DT1WRgCsmh6yddcCzr9qhr1q1zUr3JZy/0aGRnodhPWpXQuwKkCJyJSdiuxiclm4KvAW6y1X1/u65+IC5sAeFoDJyIiAkAa5VMo/ZoCnIhI2a1EBe4PgGHg3caYdxefe7m1dmYFnusJOpuYeJkCnIiICBydQhmoAiciUnorsQbu94HfX+7rLlqxRXKQ6RgBERERgCw+WoFzXW6LiIgsTeUO8u5U4AJV4ERERADIkvxFzVBTKEVESq96AS7sBDhV4ERERABcUryoqYO8RURKr3IBjrkKXNzlhoiIiKwNnQqcApyISPlVLsB1dqHUFEoREZGcS/IXNTvLDEREpLyqF+D8/NXF0CnAiYiIALi0U4GrdbchIiKyZJULcBRr4EJV4ERERHLFFEqnKZQiIqW3EufAdZcfkhKoAiciIqvOGOMDHwbOB9rAm6y1dx3jMf8L+Gdr7UdXo11HNzHRFEoRkbKrXgUOSP06NRfjnE67ERGRVXUV0LTWPgd4J/C+YzzmPcBpq9kor5hC6XxNoRQRKbtqBjivRp2IVPlNRERW12XAVwCstTcBF86/0xjzG0AGfHlVW5XqGAERkaqo3hRKIPHrNIhJ0ozQD7rdHBERWT8GgbF5H6fGmNBamxhjnga8GvgN4A8Xc7Eg8Gi1epfcKK84WmdweAh6l369tSII/GX5/qw16le5qF/lUoV+VTLApX6DupcQp46mZouIiMjqGQcG5n3sW2uT4vbrgO3AN4DTgcgYc5+19isLXSxNHaOj00tulFdU4EYnU4iWfr21otXqXZbvz1qjfpWL+lUua7lfIyMDJ34QFQ1wmV+nQUScZd1uioiIrC83Aq8ArjXGXALc0rnDWvv2zm1jzB8D+44X3paTrymUIiKVUeEAF5NoEZyIiKyu64ArjDHfBzzgamPM24C7rLXXd6tRnSmU+JUc9kVE1pVK/iXPggYNYlXgRERkVVlrM+Cax336jmM87o9XpUGFIIuIqYHnrebTiojICqjkLpSZX59bAyciIrLeeVlM4mv6pIhIFVQzwAUNGkSaQikiIkJegUu9Sk66ERFZdyoZ4FxQ1xRKERGRQuBiUk8VOBGRKqhogCvWwKkCJyIikgc4X+fqiIhUQSUDHGF+DlyiCpyIiKxzzjlCF5N5CnAiIlVQyQDngmZ+DpwqcCIiss7FqaNBQqpNTEREKqGSAc4LdA6ciIgIQJRm1EhwmkIpIlIJlQxwLmwWa+A0hVJERNa3OM2oE5MpwImIVEIlA5wXNmh4CXGadrspIiIiXRWljrqXkAWaQikiUgWVDHCETQDSuN3lhoiIiHRXXEyhRGvgREQqoZIBzq818hupApyIiKxv7SSfQulUgRMRqYRKBjgvyAOci2e63BIREZHuytfAJQpwIiIVEXa7ASvBr+VTKJ2mUIqIyDoXpY46CQTaxEREpAoqWYHrTKF0adTlloiIiHRXnGbUvRiK2SkiIlJulQxwXq0HAJfMdrklIiIi3dU5Bw5NoRQRqYRKBjg/zF9l9BJNoRQRkfUtSvIplF6oACciUgXVDHDFGjgvVQVORETWt85B3l6oKZQiIlVQyQDnOrtQJloDJyIi61uUpNS9dG6HZhERKbdKBjg6UygzBTgREVnfkmJHZj/ULpQiIlVQyQDn/Hyev6dNTEREZJ1L5wJcs8stERGR5VDNAFdU4HwdIyAiIutcViwn8LWJiYhIJVQywHXOuvEy7UIpIiLrW1bsyBzUtAZORKQKKhngXDFNRBU4ERFZ7zpnovoKcCIilVDNAFesgfNVgRMRkXUuS2IAPB3kLSJSCZUMcJ1dKAMFOBERWedcmo+FTgFORKQSqhng/JAUHz+Lu90SERGRrnLFLpToHDgRkUqoZoADIuqqwImIiKT5i5ku0DlwIiJVsGIBzhhzsTHmWyt1/RNJvBqhDvIWEZF1Lks7FThNoRQRqYJwJS5qjHk78FpgaiWuvxixVyd0CnAiIrLOFTsydzb4EhGRclupCtzdwKtW6NqLknh1AlXgRERknfOKg7xVgRMRqYYVqcBZaz9vjDl9sY8PAo9Wq3dJzxkE/mOucdCvU3Pxkq/bbY/vV1WoX+WifpVLVfslp8YrXszULpQiItWwIgHuZKWpY3R0eknXaLV6H3ONxKsRptGSr9ttj+9XVahf5aJ+lcta7tfIyEC3m7D+dDYx8bWJiYhIFVR2F8pEa+BERETwMx0jICJSJZUNcKlfp64AJyIi65yXdY4R0BRKEZEqWLEplNba+4BLVur6J5L6DWpMdOvpRURE1oROgEPnwImIVMKaWAO3EtJiExMREZHVYozxgQ8D5wNt4E3W2rvm3f9m4PWAA/6ztfaLK92mQJuYiIhUSmWnUGZ+nTqaQikiIqvqKqBprX0O8E7gfZ07jDEbgf8LeC7wYuAjxhhvpRvkd47U0TlwIiKVUN0AFzSoE+Oc63ZTRERk/bgM+AqAtfYm4MLOHdbaQ8D51toY2AKMWmtXfJDys5iUAPxgpZ9KRERWQaWnUNaJSTJHGKz4C5wiIiIAg8DYvI9TY0xorU0ArLWJMeYtwJ8AHzjRxZblnFQXkwa1Sp4NWNUzD9WvclG/yqUK/apsgHNBgwYxM5kj1IuOIiKyOsaB+Yfd+Z3w1mGt/QtjzMeBLxtjXmit/eZCF1uOc1KDLCYNa0yu0bMBl2Itn3m4FOpXuahf5bKW+7XYs1IrHODqNIhJUgfaeEtERFbHjcArgGuNMZcAt3TuMMYY4L3AvwZi8k1OspVsjHOO0MVkOsRbRKQyKhzgGjS8hDhNqHA3RURkbbkOuMIY833AA642xrwNuMtae70x5ufAD8h3ofyytfbbK9mYOHXUvYRUG5iIiFRGZZONCxoAJHEENLvbGBERWRestRlwzeM+fce8+/+EfP3bqojSjBoJmQKciEhlVHYXSoI8tKXRTJcbIiIi0h3Oke/IrCmUIiKVUdkA1zmwNI1mu9wSERGR7hhohpy/uclAX1+3myIiIsukslMovTCfQpklqsCJiMj6tbHHw880hVJEpCoqG+BcmE+hzJKoyy0RkaXypg/hGoMQlOw/odEU9Ye+S/3+bxA8ejfJlguIdj6feOtFEK6BtbnO4c0eIRi7j2D8AYKx+wnGH8CfeBj8ABc08vXEQR0XNmifdRXxjku73Wo5WWkbihc1RUSk/Cob4LxaUYGLNYVS1jmXAR545TvQPhi9h94f/hnNu64n7dvCzHlvZPbc387DXJf4U/uo3/8N6vd9ndq+n5A1h0kHd5EO7iIb2k06sJNg4kHq93+D2sM34WURWa2fdPjJ9Pz8E/T+7CO4oEG87WKiHc8jHdqFq/XNe+sFL8CLJ/GiSfxoAi+awos776fy++Lp/HYyA8ksXtLGS9uQtgm9lBY1CBq4sDkXxLxkBr89ijc7ij/7KF57DM+lj+lf2reZrH87eB7ezBG8tLhuMks69CQFuBLy0gia5T60VkREjqpsgPMDBThZR5yDdBa/PU7w6F2Eh+8gOHJH8X4PrtbP7NNfx8y5r8H1bOh2a0/In3yE3h/9fzRvvxaCBtPn/y7hodvo/8F/offH72f23N9m5vw3kvVvO7UnyFK8ZBpm2pCF4AfHflw0RTDxEMHEQ4T7f5qHsoP5sV5p/zai3S/EiyYIxh6g9sgP8ePJuS9Nhs9i5ryriXa/KK+4BfW8IvfITdQe/A71B79L/w/+yyk13wWNxwQ+FzQgbODCHlyzhas3yNozeMksXjSJlx7GS2bzxzZapBu34RotsuYwrmcD6dDuIoTuhLDnlNoka1gag3ahFBGpjMoGuKCeT0/6/I/2sGX2Sbz0KSNsGVwDU5akurIUokmWc28gf+x+GvfegD/+IH6n8hJNzVVnvGjy6O3HVVKy5jDJhqfQfspvEozdR98P/xu9P/4As2dfxcx5byTdeM7iG5JG+KP35mFm/AH8ItR40SReMpNXg5IZSGYgaORhoAgF2eBu0oFtePEMXjSO3x7Da48X1Z8MF9TzTYeCOi5oEB76JT23/i24jJmnvY7pC38P1zsCQHjgF/Tc/DF6fv6X9Pzir0g2PxOyBC9pQ9qpQkU4zwPPL96C/H0Wz7XTS9tzXduIh2sM5mGm0cI1h/BmRwnGH8SfPTL3OOf5JFsuZPKSdxKd/mLS057y2Kqmc3jtUYKx+8l6TiMb3PXE72O9j+j0FxOd/mKmAG/6IP7Mobxd0WRRXZvGyxKyxgCu1o+r9+Pqndt5aMM//p/uVquX8dHpxf98pdK8NIJQAU5EpCoqG+B2bs//8/Te2ffw9Zv+Fx+88TJGt76Al5yznQt2DLGhr05fPcAr4bQyWT3e1AFqe/+FZNN5x/4PefGYnts/S/OX/4A/uZfBXZcz+5TfJDr9Jae0zik4cieNe75E/e4vUTt0GwBZY6iotnT+E99P1jtS/Me+j6x47+oDpK0nkZ5myHo3PSZgBEfupOcXn6Rp/4me2z9LvPUisp6NRSc6j/MgmcGPJvGicbz2BF6Uv23AzV3LeQFZ/zayxhDUenD1oj1hD14yQzB+P/WHbsyrXCfJeT5t86+ZuuhtZIM7H3Nfsuk8Jq78EFOXvJOeX/wV4cFboD5I1pNPFSRs4Pw64MBleC4Dl+bhOqjhwl5crad430tPXw+zjx7Ebz96dFrh7Ciu0aJ9xtNIB3eSDe4kHdhB2noyrtlauOGeh2sOkzSHF9/X3hHSIpyKrBQvbUOgNXAiIlVR2QDnNj6FI791A037T7zYXsfLZn/E2OFPcP03L+ET2dN4xG3koL8RejYy3Fen1VOjHviEgUfoe4SBT8336KkF9NYD+ur5+956wEAjpNVTm3tTEFwD0ojw4K24Wg9p68lL2+wiS6g/8C2av/xH6vd9ba6ylQ7uJtr5PKKdzyPe/lzCg7fSc9vfUb/3q3hZQrT9Ujjn1whv/QJDN3ydrDFE+6yrmDWvwgVN/OkD+NMHj76PxqFYt+Sls5C08acPEI7eA0C85UImL/1D2me8/AlB5pS+RaedxeTl72XqkrfT/OWnadz1RYLiuYB8GiYuXzNVHyAb3J1Xpmr9NFojTNW2FGFmJ1n/lhNWgXAOb+YQwdj9+FP7IOzJg2hjKL9uYzBf65VGkEbF+q0on+Z3glCTDe5k6rI/XvL3pNHqZVqVKqm6LIZA58CJiFRFZQMcQDpyLlMj58Jz30X9we/QsJ/n1ffcwGvTr809JkrrHJ4c4dBki4SAxHmk+CTOJ3EemYMsS/Fx+Dg8HDM02OM28YDbzP1uMw+zmanmVpqNOr21gJ56QG8R/JqhTz30aYQ+9SC/XQ98PPKih+d5dKLfeDvh8FTE4amII5OzuKmD1P2EnuEdnDEyyBkbennyxj6etKGXvvq8H53L8v8ge8ETqi4n4rKMmYd+Sma/hDf+IIebuznQeBJ7a7vZF24hdgHP2D7ERbta1II1dGxg4AvbGQAAFC9JREFU2qa2/2fUHr6J2sM/oLb/J3hJvt7R+TXS4TNJNjyVZMNTSQd34qVRERTac+/znfV68spN0MSFTcL9P6N5x2cJpvaT9Wxk5hm/S3T6SwgP3krtoe/R2HMdPbf9/Vwzsubw3MYaaesMWq1eRi94B7WHvkfzjmtp3v4Zem791BOan9X68yATNouNJvJNJtLWk5k57w1EZ7yMrG/LinzrXHOYmQvezMwFb17019RavbRPNuh4Hq53hOQEYaxzZqM77qNE5FR5aZSvkxQRkUqodICb44dEu19EtPtFEE8Tjt6DP/Ew/uTDBBMPMzz5CBumD+TTrbI0n3LlErwsnVtLkxVhLsXDiw5zxdTPCbKjRxRkmUc2E+BmPPI6hkeGR0SNKXqYcPnbpGsyRYOIGrELiQmJCEnx2eVNsCt4lG3eYTa5Q4QkkEJ8IOSB/Zu4J9vC3W4z33Ub2RqMcmawn9O9/WzL9tEgX9Mz6/VwuLGDI42djPXsYqJnB1NBi0l/gMlgkElvgCnXZODwzZw9+m2e3f4Bu71DJM5nP8M8y/vyXJ/arsbdbhsP/nSEr/ub6Nmwi+27zuKMMwxh32mQx9BinRHgKCpJs/nmCUVVycuS4nua4hXfX68Z0JwYhzQijWaYnJ5hdnaGVn8PzWZPHmqKNVFePE0wtRd/ch/+1F78yb0EE4/gZREOj2TjOcyc89vE2y7GSyPCw7cTHL6d2iM/oLnnCyf1q+I8n2jX5Uw+/z1Eu18y96p1vO1iZs5/I6Qx4YGbqT/8A9LBnbTPePkTp0n6AfGuFxDvegGT7THqD3wL54dkvZuKtxGoaUc4EVklWaw1cCIiFbI+Atx8tV6SkafByNOWdJnpouoVjN1PMHYf/sRDeVDBzU1Fwznq6SyNaIoN0QRePAXRBF50JN9QIY3xsmjutmsOkw1sI+0/k2hgOzP92+jp6yXedyfbR+9l26P38sLxbxJmsyRejUPhVvYG27iFC7jbbSZNEralD7NzZi+7Z27lnNFvEngL1zUiatw58Gxu2/zviU5/CcMbNnOIWYam76V/4m56xu7kjCN3suPIfTSmbqVxZBaOADcv6Vs3Z2De7RaQOQ9/gfY6v0bWt4WsfwvJyHlEZ7yceOtFjG18FnvGQ/YcnOKue6bya/Wcz9DWGq0zaoyEM2zzDrP1tBZhvVlsltHABbX8+z+3BXu+Y1/Wv2Wu8uWc4/7D0zw4OsNTN/ezsb8BQY1k60UkWy+aa9vEbMINdxzg+lv3sXe8zZVmhF8/fytnbuzDNYZon/XK5fmGiYicAi9tz1W6RUSk/NZfgFsunp9v4tC/jXj7c1bsaZqtXqbmT11zWbHJwhCBH7AD2AEcqwUH41nc2EOE0RhhNErQPnr2U3KaIdr1QjbV+9j0mK/qBzbiuIj5E+bazpFOH2HPXbdz192W9tQYHg7fI3+PwwGTach4kr+NJQGjcUBMgO8H4NfwfB/PD2g2Ggz09XHaYD8bBwfYODRAq68Hu2+MXzx4iNsfOQxJRMOLGejrp10fpjes0ZMG9M0GMAt33z7Fg6O3zbVxsBkS+h5jMzHp43Jg6E9zxoZezt7Un7+N9NFXD/C9PgK/n8DzCOoe+w7P8otb7ueWRya4de84Y7PJ3DVOP62HC3e2uGhXi2fuGOLOg1Ncf+s+vnXXYdpJxlkjfTz3yRu47pa9XHvzIzxj+yCvOn8rLzprhEZ44umnaeaYiVOOTMccnGxzeCri0FTEocmIKM0e81jP86gHPtuHGmxv9bB9qMmWwSahX521mJlz3PzwGF/+5QG+c/dhdrR6ePHZG3nx2SNsHtB0MJFFSyNtYiIiUiGec91feRLHqRtd4kYCrVYvS73GWrRe+xUlGbftm+DHD47yyNgs01Gav8X5+zRzPGlDL2dv6uPskX7OGulj80ADz/PInGOynTA6kzA6E7N3bJY9B6fYc3CSPQcmOTIdn7B9Z2zo5enbBjlv6yA7hpvctjdvy88eGmMmPhqmBhohL3vqJn7taZsxm/oZHu7jvkfG+OIv9/OFnz/Cg6OzDDRCNvTV8vWUzpFljsxBkjniNKOdZMRp9oTQ2VEPPJq1gM4/VVesFpuNM5Ls6BcFHmwZbLJ1sMHmwSZbBhr522CDRhgwOhMzOhMzNhMzOpMwFSVs7KuzdbDJlsEGWwebbB5oMBOnPPDoDPc/Op2/PzLD4ZmYKE5JMkeaubwfDrYNNjlzpI+zirfTT+td0lrJzDnuOzLNV24/wFduP8De8TY9NZ9Ln3Qa9z86w50H8yrr07cO8uKzN3Lx6cNsHWw8dk3oSTjR72HmHA88OsMd+yeJ0oxn7Rxi+9DC56TNxim37p0gSjO2Fj+LZm2BM+ZW0Fr+uzEyMvAT4MJut6Msljw+ZikjH9lN+vx3cuTpb1m+hq0Ra/l3fSnUr3JRv8plLfdrsWOkKnCyJtVDn2fuGOKZO4ZO+mt9z2OwWWOwWWPXcA/nbRvkpU89ev+hqYi7D00xG2dk7mggSTLHcG+Np20ZZKD52H8aF+xo8dqLdpKkebD82UNjbB1s8oIzNzzhP+it3hqvuXAHr37Wdn78wCg33HGA6Sgj8PPKWVBsXhP4eRUt39zGoxb4NEOfDX11NvTV2dhXZ6S/zkAjPOYup2nmODjZ5uGxWR4eneWhsRkeHp1l30SbH93/KIemIrIFQmEz9Omt56FuoccABL7H9qEmO0/rxasHeaXS9/CL9jw0OsO1P3uYqEifoe+xqb9OLfCLN29ud9fOhj1e8TPCg3aSMdlOireUyXaCA3wPnr17mH9/2elcfuZGeorv8f1HpvnGnYf4mj3If//2PfDtvJ39jYDNAw029TfY0Fd/wnUn2gmh783tHDvUk+8kOzLUQxqnxc6zHqHv43sUoW0Ce2CK6fix5+ttG2pyUVGJPXfrAPccnuZnD41x88Nj3L5/kvRx39BWT42tgw22DTXZPtTDzlaTncM97Gj1MNJfJ0kd+yba7B2fZd/4LHvH28zGGSP9+c9/U3+DkYE6I30N6ouo5B5LnGZMRyl99YBwLW1GJCsvLdZqqwInIlIZCnCy7mwswtGpCAOf87cPcf72EwdL3/N49u5hnr178eeCnYzA99gymE+dfNYxThlI0oyDUxH7xttESUartwgvzXAudCZpxv7JNvvG8wCRV7wCdg33sHs4n5oZBv5xX61KMscDj05z54Ep9hyc4uBkmzh1JFlGlGbEaV5pdC6vHeaVxPx2PfDZOthkoBHQ3wjpa4Rs6q/zgjM3HvNntPu0Xq6+eBdXX7yLBx+d4dZ94xyYiNg/0ebARJv9E23uPjRFs5Zfb6ARsGmgQX89JHGuqD7GPDQ2w+hMzGQ7fcJzADRCn7NH+vnVczfzlM39PHVzP4Hv8eMHRvnRA6N8/c6D/POt++YeXws8ztk8wGsv3MEzdgzRXw/YO94JZW0eGZ/lzoNTfPuuw4+pmtYCj/hxpVffg1rg004eO20W8qA63FOj1VOn1RMy3Fsj8D2mo5SZOGM6TpmJUmbTjMnZhJmiYt15Tg/Y0Fdn80Bj7m24tzZ3X+eFAt/Lf9frQf7CQr24fcGOFq1ebUdfJnOH1msTExGRylCAE6mosAhHWwcXPkw8DHy2D/Ucd1rgCZ/H9zhjQx9nbOh7TKVzpe0c7mHn8Km3G2BwsIfDj06RZI4kdcRZRpo5hnvrx1xPeMaGPn7rmdtJM8cdBya5Y/8ET9rQyzmbB55QiT1/+xOfL80c+yfaPDg6w0OjecW0px6wtZjCumWwweb+BoHvMdFOODAZcXCyzcGJiAOT7blpsI9Ox+ybaHPHgbzi11cP5s6sHGyG7OyrUwN6532+EfqMzyaPCbrfv/cIs8cIigt55dO38K4rzz7Zb7N0kQvq+cH1A1u73RQREVkmCnAism75vldM9QROorAU+B7nbhng3C0DJ37w475u21CTbUNNLj5BZbYzDfjMjX0n9Ryw+Pn9zjnitFMZPVoJzFw+7TJOM6LUEaUZUZKx+zQdf1E6tV4O/84PGdq8FcZmu90aERFZBgpwIiLrlOd51MOFdi5d/c1XZGW45nB+XqeIiFSC/qKLiIiIiIiUhAKciIiIiIhISSjAiYiIiIiIlIQCnIiIiIiISEkowImIiIiIiJSEApyIiIiIiEhJKMCJiIiIiIiUhAKciIiIiIhISSjAiYiIiIiIlIQCnIiIiIiISEl4zrlutwHgIHB/txshIiIrbjcw0u1GlIjGRxGR9WNRY+RaCXAiIiIiIiJyAppCKSIiIiIiUhIKcCIiIiIiIiWhACciIiIiIlISCnAiIiIiIiIloQAnIiIiIiJSEmG3G7BUxhgf+DBwPtAG3mStvau7rTp5xpiLgf/XWnu5MeZM4G8AB9wKvNlamxlj/gj4VSAB3mqt/ZeuNfgEjDE14JPA6UADeA/wS8rfrwD4BGCAFLga8Ch5vzqMMZuAnwBXkLf7b6hGv34GjBUf3gt8DHg/eR++aq39k7L9LTHG/Cfg14A6ebu/TUV+XrI8yvY7fTwaI0vTL42RJetXFcdHqP4YWYUK3FVA01r7HOCdwPu63J6TZox5O/CXQLP41J8D77LWPo/8D98rjTEXAC8ALgb+LfChbrT1JLwGOFz04eXAX1CNfr0CwFp7KfCH5H2qQr86/6H4GDBTfKoq/WoCWGsvL96uBj4KvBq4DLi46Fdp/pYYYy4HngtcSv7z2ElFfl6yrErzO308GiNL1S+NkSXqVxXHR1gfY2QVAtxlwFcArLU3ARd2tzmn5G7gVfM+fhb5KwUAXwZeQt7Pr1prnbX2ASA0xqzlw3D/CXj3vI8TKtAva+3/AH63+HA3sJ8K9KvwZ+R/uB8pPq5Kv84Heo0xXzXGfMMY83ygYa2921rrgBuAF1OuvyUvBW4BrgP+J/BFqvPzkuVTpt/p49EYWZJ+aYwsXb+qOD7COhgjqxDgBjla+gVIjTGlmhpqrf08EM/7lFf8wwGYAIZ4Yj87n1+TrLWT1toJY8wA8DngXVSgXwDW2sQY8yngg+R9K32/jDGvBw5aa2+Y9+nS96swTT7wvhS4Bvjr4nMdC/VtLf8t2Ug+gP4meZ/+AfAr8vOS5VOm3+kFaYwsT79AYyQl6hfVHB9hHYyRVQhw48DAvI99a23SrcYsk2ze7QFglCf2s/P5NcsYsxP4JvB31tpPU5F+AVhrfwc4m3yuf8+8u8rarzcAVxhjvgU8A/hbYNO8+8vaL4A9wN8Xr7DtIf9jfdq8+xfq21r+W3IYuMFaG1lrLTDLYwedMv+8ZPmU6Xf6ZFRiLNEYWap+VXWMrOL4COtgjKxCgLsR+BUAY8wl5CXTsvtZMX8X8rnx3yXv50uNMb4xZhf5P55D3WrgiRhjNgNfBd5hrf1k8ekq9Ou1xcJYyF+lyoAfl71f1trnW2tfYK29HLgZeB3w5bL3q/AGivn6xphtQC8wZYx5sjHGI3/lsdO3svwt+R7wMmOMV/SpD/h6RX5esnzK9Dt9MqowlmiMLFG/KjxGVnF8hHUwRq7l8udiXUf+qsj3yRclXt3l9iyH/wf4hDGmDtwOfM5amxpjvgv8gDx4v7mbDVyEPwCGgXcbYzrz/H8f+EDJ+/UF4K+NMd8BasBbyftS9p/XsVTh9xDgr4C/McZ8j3z3qTeQ/6fiH4CAfP77D40xP6Ikf0ustV8s1ir8C0d/DvdSjZ+XLJ8qjo9Qjb9NGiPL1a9jqcLvYeXGR1gfY6TnnDvxo0RERERERKTrqjCFUkREREREZF1QgBMRERERESkJBTgREREREZGSUIATEREREREpCQU4ERERERGRklCAE1lmxpjLjTGf6XY7REREVpsx5h3GmL3GmGa32yJSVQpwIiIiIrJcfhv4DPBvu90QkaqqwkHeImueMeYK4D3ALHCY/LDMGvBZ8hdSasA1wJ3AtcAQ0AO83Vr7rS40WURE5KQYYy4H7gY+Cvw9+SHRFwPvJz8A+mHygHfeMT73ZeAaa+0dxphrgC3A3wD/k3zc/BLwQ+CPiqfrBV5nrd1jjHkXcBX5/2s/Qn4o9VnW2v9ojAmAm4ELrbXtFf0GiKwSVeBEVpgxxgM+DrzKWvsC4NvAu4BnA2PAy4HfAwaBJ5MPWq8AXk0+QImIiJTBm4C/tNZaoF2Et48DV1trLwa+Bjx1gc8tZAtwpbX2T4FzgddYa18EXA/8pjHmmeTj6MXAc4FzgH8ErirC28uAbyq8SZWoAiey8jYC49bah4uPvwP8V+DtwFnAPwMx8B5r7W3GmA+RDz414ANdaK+IiMhJMcYMA78CbDLG/N/kM0neAmy21t4OYK39cPHYY31u/uW8ebfvtdZGxe2HgQ8YYyaB7cCNgAH+xVqbAtPA7xfX+zbwUuBq4D8ve4dFukgVOJGVdwgYNMZsLT5+AbAHuBzYa629knx65X81xjwdGLDW/irwO8AHu9BeERGRk/Ua4K+stVdaa19GXhG7EpgxxpwFcxuc/DrwyDE+Nwt0xskL5l03m3f7L8krd68HHiEPencAFxhjfGNMzRjzv40xDeAT5BXBTdbaX6xQn0W6QhU4kZVxpTHmx/M+fi/wBWNMBjwKvJ58jv5njTFvBVLyVwjvBP7IGPM6IAL+cFVbLSIicmreBLy284G1dtoY83lgP/DJYvzbC/x34KFjfK4NfMgY8yB5pe1Y/g74oTHm0eK626y1NxtjvkJejfOBjxTTJX9ojDkT+NAK9FWkqzznXLfbICIiIiKybIwxPnmoe6m1drzb7RFZTppCKSIiIiKVYYx5EvBT4G8V3qSKVIETEREREREpCVXgRERERERESkIBTkREREREpCQU4EREREREREpCAU5ERERERKQkFOBERERERERKQgFORERERESkJP5/c0iq/3feNF0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Loss after each Epoch')\n",
    "plt.plot(history.epoch[::10], history.history['loss'][::10], label='Train')\n",
    "plt.plot(history.epoch[::10], history.history['val_loss'][::10], label='Test')\n",
    "plt.legend(['Train', 'Test'],loc='upper right', title='Sample', facecolor='white',fancybox=True)\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Epochs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracy after each Epoch')\n",
    "plt.plot(history.epoch[::10], history.history['acc'][::10], label='Train')\n",
    "plt.plot(history.epoch[::10], history.history['val_acc'][::10], label='Test')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Epochs')\n",
    "plt.legend(['Train', 'Test'], loc='upper left', title='Sample', facecolor='white', fancybox=True)\n",
    "\n",
    "\n",
    "plt.savefig('data/results/loss_acc.jpg', quality=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.95      0.85       165\n",
      "           1       0.55      0.20      0.29        55\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       220\n",
      "   macro avg       0.67      0.57      0.57       220\n",
      "weighted avg       0.72      0.76      0.71       220\n",
      "\n",
      "[[156   9]\n",
      " [ 44  11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(test_target, saved_model.predict_classes(test_features)))\n",
    "print(confusion_matrix(test_target, saved_model.predict_classes(test_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.96      0.86       627\n",
      "           1       0.75      0.31      0.44       252\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       879\n",
      "   macro avg       0.76      0.63      0.65       879\n",
      "weighted avg       0.77      0.77      0.74       879\n",
      "\n",
      "[[602  25]\n",
      " [175  77]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_target, saved_model.predict_classes(train_features)))\n",
    "print(confusion_matrix(train_target, saved_model.predict_classes(train_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019 Wimbledon Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jugalmarfatia/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (27,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_2019 = pd.read_csv('data/wimbledon_2019.csv')\n",
    "df_raw = pd.read_csv('data/mens/combined_raw_data.csv')\n",
    "\n",
    "df_2019['Date'] = '2019/07/07'\n",
    "df_2019['Surface'] = 'Grass'\n",
    "df_2019['diff_rank'] = df_2019['player_0_rank'] - df_2019['player_1_rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating features to make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Player Career Stats All Surface\n",
      "Creating Player Career Stats on Grass/Clay/Hard\n",
      "Creating Player Career Stats All Surface Last 52 Weeks\n",
      "Creating Player Career Stats on Grass/Clay/Hard Last 60 Weeks\n",
      "Creating Player Head to Head Career Stats All Surface\n",
      "Creating Player Head to Head Career Stats On Grass\n",
      "Creating Difference Variables\n"
     ]
    }
   ],
   "source": [
    "df_2019 = create_features(df_2019, df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predictions\n",
    "    - Outcome 0 indicates player_0 will win and outcome 1 indicates player_1 will win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Round</th>\n",
       "      <th>player_0</th>\n",
       "      <th>player_1</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Round of 16</td>\n",
       "      <td>Djokovic N.</td>\n",
       "      <td>Humbert U.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.887655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Round of 16</td>\n",
       "      <td>Goffin D.</td>\n",
       "      <td>Verdasco F.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Round of 16</td>\n",
       "      <td>Raonic M.</td>\n",
       "      <td>Pella G.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.850343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Round of 16</td>\n",
       "      <td>Bautista Agut R.</td>\n",
       "      <td>Paire B.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.806741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Round of 16</td>\n",
       "      <td>Querrey S.</td>\n",
       "      <td>Sandgren T.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Round of 16</td>\n",
       "      <td>Nadal R.</td>\n",
       "      <td>Sousa J.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.899063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Round of 16</td>\n",
       "      <td>Nishikori K.</td>\n",
       "      <td>Kukushkin M.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.846639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Round of 16</td>\n",
       "      <td>Federer R.</td>\n",
       "      <td>Berrettini M.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.860115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Quarter</td>\n",
       "      <td>Djokovic N.</td>\n",
       "      <td>Goffin D.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.838845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Quarter</td>\n",
       "      <td>Raonic M.</td>\n",
       "      <td>Bautista Agut R.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Quarter</td>\n",
       "      <td>Nadal R.</td>\n",
       "      <td>Querrey S.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.827736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Quarter</td>\n",
       "      <td>Federer R.</td>\n",
       "      <td>Nishikori K.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.877711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Semis</td>\n",
       "      <td>Djokovic N.</td>\n",
       "      <td>Bautista Agut R.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.714795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Semis</td>\n",
       "      <td>Nadal R.</td>\n",
       "      <td>Federer R.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.503486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Finals</td>\n",
       "      <td>Djokovic N.</td>\n",
       "      <td>Federer R.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.697209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Finals</td>\n",
       "      <td>Djokovic N.</td>\n",
       "      <td>Nadal R.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.588448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Round          player_0          player_1  prediction  probability\n",
       "0   Round of 16       Djokovic N.        Humbert U.           0     0.887655\n",
       "1   Round of 16         Goffin D.       Verdasco F.           0     0.557724\n",
       "2   Round of 16         Raonic M.          Pella G.           0     0.850343\n",
       "3   Round of 16  Bautista Agut R.          Paire B.           0     0.806741\n",
       "4   Round of 16        Querrey S.       Sandgren T.           0     0.818642\n",
       "5   Round of 16          Nadal R.          Sousa J.           0     0.899063\n",
       "6   Round of 16      Nishikori K.      Kukushkin M.           0     0.846639\n",
       "7   Round of 16        Federer R.     Berrettini M.           0     0.860115\n",
       "8       Quarter       Djokovic N.         Goffin D.           0     0.838845\n",
       "9       Quarter         Raonic M.  Bautista Agut R.           1     0.909341\n",
       "10      Quarter          Nadal R.        Querrey S.           0     0.827736\n",
       "11      Quarter        Federer R.      Nishikori K.           0     0.877711\n",
       "12        Semis       Djokovic N.  Bautista Agut R.           0     0.714795\n",
       "13        Semis          Nadal R.        Federer R.           1     0.503486\n",
       "14       Finals       Djokovic N.        Federer R.           0     0.697209\n",
       "15       Finals       Djokovic N.          Nadal R.           0     0.588448"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_16 = df_2019[features_list]\n",
    "\n",
    "df_2019['prediction'] = saved_model.predict_classes(features_16)\n",
    "df_2019['probability'] = 1 - np.abs(df_2019.prediction - saved_model.predict_proba(features_16).flatten())\n",
    "\n",
    "df_2019[['Round', 'player_0', 'player_1', 'prediction', 'probability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
